#!/bin/bash
# Script de verificaciÃ³n de estado para Notebook Architect Web (Linux/macOS)

echo "ğŸ”§ VerificaciÃ³n de Estado - Notebook Architect Web"
echo "================================================="
echo ""

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# FunciÃ³n para mostrar estado
show_status() {
    if [ $1 -eq 0 ]; then
        echo -e "${GREEN}âœ… $2${NC}"
    else
        echo -e "${RED}âŒ $2${NC}"
    fi
}

# Verificar herramientas necesarias
echo -e "${BLUE}ğŸ“‹ Verificando herramientas...${NC}"
command -v python3 >/dev/null 2>&1
show_status $? "Python3 disponible"

command -v curl >/dev/null 2>&1
show_status $? "cURL disponible"

echo ""

# Verificar puertos
echo -e "${BLUE}ğŸŒ Verificando puertos...${NC}"

# Puerto 1234 (LM Studio)
if nc -z localhost 1234 2>/dev/null; then
    echo -e "${GREEN}âœ… Puerto 1234 (LM Studio): Abierto${NC}"
else
    echo -e "${RED}âŒ Puerto 1234 (LM Studio): Cerrado${NC}"
fi

# Puerto 11434 (Ollama)
if nc -z localhost 11434 2>/dev/null; then
    echo -e "${GREEN}âœ… Puerto 11434 (Ollama): Abierto${NC}"
else
    echo -e "${RED}âŒ Puerto 11434 (Ollama): Cerrado${NC}"
fi

echo ""

# Verificar servicios
echo -e "${BLUE}ğŸ”— Verificando servicios...${NC}"

# LM Studio
if curl -s http://localhost:1234/v1/models >/dev/null 2>&1; then
    echo -e "${GREEN}âœ… LM Studio: Disponible${NC}"
    models=$(curl -s http://localhost:1234/v1/models | grep -o '"id":[^,]*' | wc -l)
    echo -e "   ğŸ“Š Modelos cargados: $models"
else
    echo -e "${RED}âŒ LM Studio: No disponible${NC}"
fi

# Ollama
if curl -s http://localhost:11434/api/version >/dev/null 2>&1; then
    echo -e "${GREEN}âœ… Ollama: Disponible${NC}"
    version=$(curl -s http://localhost:11434/api/version | grep -o '"version":"[^"]*' | cut -d'"' -f4)
    echo -e "   ğŸ“Š VersiÃ³n: $version"
    
    # Listar modelos de Ollama
    if command -v ollama >/dev/null 2>&1; then
        models=$(ollama list | tail -n +2 | wc -l)
        echo -e "   ğŸ“Š Modelos instalados: $models"
    fi
else
    echo -e "${RED}âŒ Ollama: No disponible${NC}"
fi

echo ""

# Verificar servidor web
echo -e "${BLUE}ğŸŒ Verificando aplicaciÃ³n web...${NC}"
if curl -s http://localhost:8080 >/dev/null 2>&1; then
    echo -e "${GREEN}âœ… Servidor web en puerto 8080: Funcionando${NC}"
    echo -e "   ğŸ”— AplicaciÃ³n: http://localhost:8080"
    echo -e "   ğŸ”§ Debug: http://localhost:8080/debug.html"
else
    echo -e "${RED}âŒ Servidor web en puerto 8080: No disponible${NC}"
    echo -e "${YELLOW}ğŸ’¡ Ejecuta: python3 -m http.server 8080${NC}"
fi

echo ""

# Instrucciones finales
echo -e "${BLUE}ğŸ’¡ Instrucciones:${NC}"
echo "1. Si LM Studio estÃ¡ âŒ: Abre LM Studio â†’ Local Server â†’ Start Server"
echo "2. Si Ollama estÃ¡ âŒ: Ejecuta 'ollama serve' en terminal"
echo "3. Si el servidor web estÃ¡ âŒ: Ejecuta 'python3 -m http.server 8080'"
echo "4. Para diagnÃ³stico avanzado: abre http://localhost:8080/debug.html"

echo ""
echo -e "${GREEN}ğŸš€ Â¡Todo listo? Abre http://localhost:8080 y comienza a crear!${NC}"